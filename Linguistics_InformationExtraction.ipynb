{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNywRttQpnc8+nsC87VeJla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herzeg137/NLP/blob/main/Linguistics_InformationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parts of Linguistics:***\n",
        "\n",
        "POS tags + yozildi\n",
        "\n",
        "Dependency Parsing + yozildi\n",
        "\n",
        "Constituency Parsing + yozildi\n",
        "\n",
        "Syntatic Parsing + yozildi\n",
        "\n",
        "Sentiment Analysis + yozildi\n",
        "\n",
        "Semantic Analysis -\n",
        "\n",
        "Lexical Semantics Analysis -\n",
        "\n",
        "Compositional Semantics Analysis - \n",
        "\n",
        "Chunking + \n",
        "\n",
        "Coreference Resolution -\n",
        "\n",
        "Anaphore Resolution - \n",
        "\n",
        "\n",
        "Entity Extraction/NER + yozildi\n",
        "\n",
        "Named Entity Disambiguation(NED) -  / Entity Linking (NEL) +\n",
        "\n",
        "Relation Extraction\n",
        "\n",
        "Semantic Relationship\n",
        "\n",
        "Knowledge Graphs\n",
        "\n",
        "Sentence Boundary Detection (SBD) +\n",
        "\n",
        "Synonyms/antonyms\n",
        "\n",
        "\n",
        "\n",
        "**addition:**\n",
        "\n",
        "pipeline in HuggingFace\n",
        "\n",
        "pipeline in SpaCy \n",
        "\n"
      ],
      "metadata": {
        "id": "uszbMeoL2qNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-Speech\n",
        "https://suneelpatel18.medium.com/nlp-pos-part-of-speech-tagging-chunking-f72178cc7385\n",
        "\n",
        "CC -  coordinating conjunction \n",
        "\n",
        "\n",
        "CD -  cardinal digit \n",
        "\n",
        "\n",
        "DT determiner \n",
        "\n",
        "EX -  existential there (like: “there is” … think of it like “there exists”) \n",
        "\n",
        "FW -  foreign word \n",
        "\n",
        "IN -  preposition/subordinating conjunction \n",
        "\n",
        "JJ -  adjective – ‘big’ \n",
        "\n",
        "JJR -  adjective, comparative – ‘bigger’ \n",
        "\n",
        "JJS -  adjective, superlative – ‘biggest’ \n",
        "\n",
        "LS -  list marker 1) \n",
        "\n",
        "MD -  modal – could, will \n",
        "\n",
        "NN -  noun, singular ‘- desk’ \n",
        "\n",
        "NNS -  noun plural – ‘desks’ \n",
        "\n",
        "NNP -  proper noun, singular – ‘Harrison’ \n",
        "\n",
        "NNPS -  proper noun, plural – ‘Americans’ \n",
        "\n",
        "PDT -  predeterminer – ‘all the kids’ \n",
        "\n",
        "POS -  possessive ending parent’s \n",
        "\n",
        "PRP -  personal pronoun –  I, he, she \n",
        "\n",
        "PRP -  possessive pronoun – my, his, hers \n",
        "\n",
        "RB -  adverb – very, silently, \n",
        "\n",
        "RBR -  adverb, comparative – better \n",
        "\n",
        "RBS -  adverb, superlative – best \n",
        "\n",
        "RP - particle – give up \n",
        "\n",
        "\n",
        "TO – to go ‘to’ the store. \n",
        "\n",
        "UH -  interjection – errrrrrrrm \n",
        "\n",
        "VB -  verb, base form – take \n",
        "\n",
        "VBD -  verb, past tense – took \n",
        "\n",
        "VBG -  verb, gerund/present participle – talking \n",
        "\n",
        "VBN -  verb, past participle – talken \n",
        "\n",
        "VBP -  verb, sing. present, non-3d – take \n",
        "\n",
        "VBZ -  verb, 3rd person sing. present – takes \n",
        "\n",
        "WDT -  wh-determiner – which \n",
        "\n",
        "WP -  wh-pronoun – who, what \n",
        "\n",
        "WP -  possessive wh-pronoun, eg- whose \n",
        "\n",
        "WRB -  wh-adverb, eg- where, when *italicized text*"
      ],
      "metadata": {
        "id": "wEAcf3Kp-t3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#POS1\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
        "    \"Sukanya is getting married next year.     \" \\\n",
        "    \"It's both exciting and frightening. \" \\\n",
        "    \"But friendship is a sacred bond between people.\" \\\n",
        "    \"It is a special kind of love between us. \" \\\n",
        "    \"Many of you must have tried searching for a friend \"\\\n",
        "    \"but never found the right one.\"\n",
        " \n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        " \n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "     \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        " \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        " \n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)"
      ],
      "metadata": {
        "id": "I87VxuHG3hpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#POS2\n",
        "\n",
        "#Installing\n",
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!pip install beautifultable\n",
        "\n",
        "from beautifultable import BeautifulTable\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "input = \"Sukanya, Rajib and Naba're my good friends.\"\n",
        "doc = nlp(input)\n",
        "table = BeautifulTable()\n",
        "table.columns.header = ['text', 'POS', 'tag', 'Dep', 'Shape', 'is_alpha', 'is_stop']\n",
        "for token in doc:\n",
        "  table.rows.append([token.text, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop])\n",
        "print(table)"
      ],
      "metadata": {
        "id": "xKuXzTS6n42f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#POS3 \n",
        "\n",
        "#Installing\n",
        "#!pip install textblob\n",
        "\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "wiki = TextBlob(\"Here we go\")\n",
        "wiki.tags"
      ],
      "metadata": {
        "id": "SOuQbW-t6eXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing\n",
        "\n",
        "# Explaining Dependeny Parsing:**bold text** https://www.megaputer.com/why-we-use-dependency-parsing/\n",
        "\n",
        "\n",
        "# **Dependency Parsing parts(nsubj, aux and others): **https://universaldependencies.org/u/dep/all.html\n",
        "\n",
        "\n",
        "\n",
        "# **Explaining Constituency parsing:** https://paperswithcode.com/task/constituency-parsing\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/\n",
        "\n",
        "Note: Constituency Parsing gapni turlarga bo'ladi , va u turlar ham o'zining turlariga bo'linadi, xuddi POS dagi kabi, faqat CP explaine qilib beradi, Masalan I yoki It shaxslari NP ya'ni Noun phrase turiga tegishli, va NP ning turlaridan biri Bo'lgan PRP bu yerda It va I bo'ladi. POS dagi xususiyatlar qanaqa qismlarga kirishini bilish uchun bu ajoyib narsa\n",
        "\n",
        "\n",
        "# **Explaining Syntatical Parsing:**https://www.analyticsvidhya.com/blog/2022/03/syntactical-parsing-in-nlp/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iVd5-3SE5E62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependency parsing1\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I want to eat apple anyone\"\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style ='dep', jupyter = True) "
      ],
      "metadata": {
        "id": "VjIn78l95Iwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependency parsing2\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I want to eat apple anyone\"\n",
        "doc = nlp(text)\n",
        "options = {\"compact\" : True, \"distance\" : 200, \"bg\" : \"#4D5656\", \"color\" : \"#ECF0F1\", \"font\" : \"Poppins\" } #https://htmlcolorcodes.com/ << for getting codes\n",
        "displacy.render(doc, style ='dep', jupyter = True, options=options)\n",
        "\n",
        "#Explaining:\n",
        "     #1.\"compact\" : True -  bog'lanishlarni bizga to'g'ri chiziq orqali ko'rsatadi.\n",
        "     #2.\"distance\" : 200 -  grafikni o'lchamini o'zgartiradi\n",
        "     #3.\"bg\" : \"#4D5656\" - background ya'ni orqa fonni rangini o'zgartiradi.\n",
        "     #4.\"color\" : \"#ECF0F1\" - chiziqlarni rangini o'zgaritradi.\n",
        "     #5.\"font\" : \"Poppins\" - font ya'ni biz yozgan gapning grafikdagi shriftini o'zgartiradi.\n",
        "     #6.jupyter = True - grafik shaklida qo'yish"
      ],
      "metadata": {
        "id": "HCRi8R6h5I13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constituency Parsing with spacy and benepar\n",
        "\n",
        "#Installing\n",
        "!pip install benepar\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "import benepar\n",
        "import spacy\n",
        "benepar.download('benepar_en3')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('benepar', config = {'model' : 'benepar_en3'})\n",
        "doc = nlp('I will fuck you')\n",
        "sent = list(doc.sents)[0]\n",
        "print(sent._.parse_string)\n",
        "#print(sent._.labels)"
      ],
      "metadata": {
        "id": "OwlnRdSH5UPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constituency Parsing with stanza\n",
        "\n",
        "#Installing\n",
        "#!pip install stanza\n",
        "\n",
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')\n",
        "doc = nlp('This is a test')\n",
        "for sentence in doc.sentences:\n",
        "    print(sentence.constituency)\n",
        "#tree.children"
      ],
      "metadata": {
        "id": "ScWTYpvo5j8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Syntatic Parsing1\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I will fuck you\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print(token.text, '=>', token.dep_)"
      ],
      "metadata": {
        "id": "7TfteH-X5ngk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Syntatic Parsing2 with vizualizing\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I will fuck you\"\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, jupyter = True)\n",
        "  "
      ],
      "metadata": {
        "id": "96M-Emc-oqS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "\n",
        "Semantic Analysis: https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-semantic-analysis/#:~:text=Semantic%20analysis%20analyzes%20the%20grammatical,language%20processing%20(NLP)%20systems.\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/understanding-semantic-analysis-nlp/#:~:text=Semantic%20Analysis%20is%20a%20subfield,process%20to%20us%20as%20humans.\n",
        "\n",
        "https://monkeylearn.com/blog/semantic-analysis/\n"
      ],
      "metadata": {
        "id": "m2Xr3OTz5Vks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment Analysis\n",
        "\n",
        "#installing \n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "output = classifier([\"I want fuck anyone\", \"I love you\"])\n",
        "for result in output:\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "CRpWPrzJcewU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Semantic Analysis"
      ],
      "metadata": {
        "id": "dfq51ZZ_5ytV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lexical Semantics\n",
        "#Read this article\n",
        "https://medium.com/swlh/nlp-lexical-semantics-866545b04f81"
      ],
      "metadata": {
        "id": "-rRmgM0U50cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognation\n",
        "\n",
        "NER ni biz mana bularni aniqlash uchun ishlatamiz:\n",
        "\n",
        "\n",
        "CARDINAL - boshqa turga kirmaydigan raqamlar\n",
        "\n",
        "\n",
        "DATE - sana\n",
        "\n",
        "\n",
        "EVENT - voqealar(urushlar, janglar, battllar, sport)\n",
        "\n",
        "\n",
        "\n",
        "FAC - qurilishlar, ko'priklar, aeroportlar, magistrallar\n",
        "\n",
        "\n",
        "GPE - shaharlar, shtatlar, davlatlar\n",
        "\n",
        "LANGUAGE - til\n",
        "\n",
        "LAW - qununiy hujjatlar\n",
        "\n",
        "LOC - tog' tizmalari, suv havzalari\n",
        "\n",
        "MONEY - Pul \n",
        "\n",
        "NORP - aholining dinlari, millatlari\n",
        "\n",
        "ORDINAL - birinchi, ikkinchi, uchinchi va h.k\n",
        "\n",
        "ORG - kompaniyalar, agentliklar, institutlar va h.k\n",
        "\n",
        "PERCENT - %(foizlar)\n",
        "\n",
        "PERSON - odamlar\n",
        "\n",
        "PRODUCT - mahsulotlar, ovqatlar, mashinalar va h.k\n",
        "\n",
        "QUANTITY - vazn yoki masofa bo'yicha o'lchovlar\n",
        "\n",
        "TIME - kundan kichik bo'lgan vaqt\n",
        "\n",
        "WORK_OF_ART - kitoblarning nomlari, qo'shiqlar va h.k"
      ],
      "metadata": {
        "id": "quOGrAed5P9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NER o'z ichiga nimalarni olishini ko'rish\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.pipe_labels['ner']\n",
        "#Bu yordamida biz NER ga nimalar kirishini ko'rishimiz mumkin"
      ],
      "metadata": {
        "id": "NQyViPtY6z1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER1 \n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.pipe_names\n",
        "doc = nlp(\"I will buy Apple for 100$ billion\" )\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \"|\", ent.label_)"
      ],
      "metadata": {
        "id": "xV54aT4_z_rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER2 \n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.pipe_names\n",
        "doc = nlp(\"I will buy Apple for 100$ billion\" )\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \"|\", ent.label_, \"|\" , spacy.explain(ent.label_))\n",
        "\n",
        "  #spacy.explain(ent.label_) - bizga to'liqroq ma'lumot beradi.ORG ga nimalar kirishini va h.k.\n",
        "  #\"|\" - ajratish uchun osh."
      ],
      "metadata": {
        "id": "_5jNxgInz_tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER3\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style = \"ent\", jupyter = True)\n",
        "\n",
        "#displacy.render - render qilish funksiyasi\n",
        "#style -  ko'rinishning stili\n",
        "#ent - entity visualizer\n",
        "#jupyter = True - bizga natijani ko'rsatadi"
      ],
      "metadata": {
        "id": "DkTbSv7Q0-nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "0WT6078P6FMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the library/textblob\n",
        "!pip install textblob"
      ],
      "metadata": {
        "id": "TK0P1pGNIJNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunk2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"the little yellow dog barked at the cat\"\n",
        "grammar = ('''\n",
        "    NP: {<DT>?<JJ>*<NN>} # NP\n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar)\n",
        "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "tagged\n",
        "tree = chunkParser.parse(tagged)\n",
        "for subtree in tree.subtrees():\n",
        "    print(subtree)"
      ],
      "metadata": {
        "id": "p7h4lI-FLN2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Libraries\n",
        "from nltk.chunk.regexp import ChunkString\n",
        "from nltk.chunk.regexp import ChunkRule\n",
        "from nltk.chunk.regexp import ChinkRule\n",
        "from nltk.chunk.api import ChunkParserI\n",
        "from nltk.tree import Tree\n",
        "  \n",
        "# ChunkString() starts with the flat tree\n",
        "tree = Tree('S', [('the', 'DT'), ('book', 'NN'),\n",
        "               ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])\n",
        "  \n",
        "# Initializing ChunkString()\n",
        "chunk_string = ChunkString(tree)\n",
        "print (\"Chunk String : \", chunk_string)\n",
        "  \n",
        "# Initializing ChunkRule\n",
        "chunk_rule = ChunkRule('<DT><NN.*><.*>*<NN.*>', 'chunk determiners and nouns')\n",
        "chunk_rule.apply(chunk_string)\n",
        "print (\"\\nApplied ChunkRule : \", chunk_string)\n",
        "  \n",
        "# Another ChinkRule\n",
        "ir = ChinkRule('<VB.*>', 'chink verbs')\n",
        "ir.apply(chunk_string)\n",
        "print (\"\\nApplied ChinkRule : \", chunk_string, \"\\n\")\n",
        "  \n",
        "# Back to chunk sub-tree\n",
        "chunk_string.to_chunkstruct()"
      ],
      "metadata": {
        "id": "lQCqIT9TaONH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Conference Resolution\n",
        "\n",
        "https://neurosys.com/blog/intro-to-coreference-resolution-in-nlp"
      ],
      "metadata": {
        "id": "zWyaBjsc6JUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simplified AllenNLP’s coref_resolved method implementation\n",
        "\n",
        "def coref_resolved(self, document: str) -> str:\n",
        "    spacy_document = self._spacy(document)  # spaCy Doc\n",
        "    clusters = self.predict(document).get(\"clusters\")\n",
        "    return self.replace_corefs(spacy_document, clusters)"
      ],
      "metadata": {
        "id": "cADzneKuT-jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
        "    resolved = list(tok.text_with_ws for tok in document)\n",
        "    for cluster in clusters:\n",
        "# all logic happens here"
      ],
      "metadata": {
        "id": "bYzug2orT-tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
        "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
        "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
        "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
        "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
        "    return span_noun_indices\n",
        "\n",
        "\n",
        "def replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
        "    resolved = list(tok.text_with_ws for tok in document)\n",
        "\n",
        "    for cluster in clusters:\n",
        "        noun_indices = get_span_noun_indices(document, cluster)\n",
        "        if noun_indices:  # if there are any noun phrases in the cluster\n",
        "            # all logic happens here"
      ],
      "metadata": {
        "id": "p7lH7sqVT-v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "text = 'Hello, my name is Guasto Riko'\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(doc._.coref_clusters)\n",
        "print(doc._.coref_resolved)"
      ],
      "metadata": {
        "id": "8eZqjP1RT-1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python = 3.5"
      ],
      "metadata": {
        "id": "I4jbhDyaT-4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/huggingface/neuralcoref.git\n"
      ],
      "metadata": {
        "id": "znMKHLzvT-7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity Linking"
      ],
      "metadata": {
        "id": "TlQZoNOD6xNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NEL\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Ada Lovelace was born in London\")\n",
        "\n",
        "# Document level\n",
        "ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\n",
        "print(ents)  # [('Ada Lovelace', 'PERSON', 'Q7259'), ('London', 'GPE', 'Q84')]\n",
        "\n",
        "# Token level\n",
        "ent_ada_0 = [doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_]\n",
        "ent_ada_1 = [doc[1].text, doc[1].ent_type_, doc[1].ent_kb_id_]\n",
        "ent_london_5 = [doc[5].text, doc[5].ent_type_, doc[5].ent_kb_id_]\n",
        "print(ent_ada_0)  # ['Ada', 'PERSON', 'Q7259']\n",
        "print(ent_ada_1)  # ['Lovelace', 'PERSON', 'Q7259']\n",
        "print(ent_london_5)"
      ],
      "metadata": {
        "id": "JLk7p1dZCjnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Boundary Detection (SBD)"
      ],
      "metadata": {
        "id": "s8rU6rC47T03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SBD1\n",
        "#Installing\n",
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!pip install beautifultable\n",
        "#!python -m downlaod en-core_web_sm\n",
        "\n",
        "import spacy\n",
        "nlp  = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Do not fuck!\"\n",
        "doc = nlp(text)\n",
        "sentence =  list(doc.sents)\n",
        "for sent in sentence:\n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "lYkuBgf5Cj3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBD2\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "get_sentences(\"We are reading a book. Do you know who is \"\\\n",
        "              \"the publisher? It is Packt. Packt is based \"\\\n",
        "              \"out of Birmingham.\")\n"
      ],
      "metadata": {
        "id": "cz7Gb0pXCj7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph"
      ],
      "metadata": {
        "id": "SglbzNqS-ngi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/"
      ],
      "metadata": {
        "id": "9k07PyYbHzSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synonyms/antonyms\n",
        "\n",
        "tutorialspoint.com/how-to-get-synonyms-antonyms-from-nltk-wordnet-in-python"
      ],
      "metadata": {
        "id": "aEawa7TYpSfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paramethrs of wordnet\n",
        "\n",
        "\n",
        "#WordNet ni yuklash\n",
        "import nltk \n",
        "nltk.download('wordnet') \n",
        "\n",
        "#Import qilish\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "#dicipline so'zini sysetini topish\n",
        "from nltk.corpus import wordnet \n",
        "syns = wordnet.synsets(\"Discipline\") \n",
        "print(syns[0].name()) \n",
        "#output >> discipline.n.01 \n",
        "\n",
        "#Root word ni o'zgartirish\n",
        "print(syns[0].lemmas()[0].name()) \n",
        "#output >> Dicipline\n",
        "#Note: Synonim yoki antonimni topish uchun bizga shu korinishdagi kerak\n",
        "\n",
        "#So'zning Definition ini olish uchun\n",
        "print(syns[0].definition()) \n",
        "\n",
        "#Gaplarda misol ko'rish uchun\n",
        "print(syns[0].examples())"
      ],
      "metadata": {
        "id": "iyrrOLSnsYUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impelmentation\n",
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets('active'):\n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))\n",
        "\n",
        "#NOTE: Agarda so'zning synonim va antonim larnini topish kerak bo'lsa sayt\n",
        "#https://www.thesaurus.com/browse/active"
      ],
      "metadata": {
        "id": "d5fz1iXGwKXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfAYpO2FwKcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}